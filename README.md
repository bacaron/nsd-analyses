# Natural Scenes Dataset - Diffusion data

This is the code repository describing how to download the diffusion derivatives from brainlife.io and everything done to the data before upload to aws
<!--
#![fig1](./reports/figures/fig1.png)

#![fig2](./reports/figures/fig2.png)
-->

### Authors 

- Brad Caron (bacaron@iu.edu)

### Acknowledgements  

This research was supported by NSF OAC-1916518, NSF IIS-1912270, NSF, IIS-1636893, NSF BCS-1734853, NIH NIDCD 5R21DC013974-02, NIH 1R01EB029272-01, NIH NIMH 5T32MH103213, the Indiana Spinal Cord and Brain Injury Research Fund, Microsoft Faculty Fellowship, the Indiana University Areas of Emergent Research initiative “Learning: Brains, Machines, Children.” We thank Soichi Hayashi, and David Hunt for contributing to the development of brainlife.io, Craig Stewart, Robert Henschel, David Hancock and Jeremy Fischer for support with jetstream-cloud.org (NSF ACI-1445604). We also thank The Indiana University Lawrence D. Rink Center for Sports Medicine and Technology and Center for Elite Athlete Development for contributing funding to athletic scientific research and for the development of a new research facility.

### Data availability

Data used in this project can be found at the accompanying [brainlife.io project](https://brainlife.io/project/5cb8973c71a8630036207a6a).

### Project Directory Organization

For a better understanding of how this code was run locally, here is the local directory structure:

	.
	├── analyzeWmGmData.py
	├── bl_download.sh
	├── configs
	│   ├── config.json
	│   ├── distinct_colors.txt
	│   └── labels.txt
	├── main
	├── README.md
	└── utils
	    ├── analyses.py
	    ├── compile_data.py
	    └── plotting.py
	
	2 directories, 10 files

<!--
<sub> This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1342962. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation. </sub>
-->

### 1. Downloading data

First, the data was downloaded from brainlife.io following processing. To download the data, one must have either the brainlife.io command-line interface installed (XX ref), or singularity installed. If running through singularity and dockers, please add this function to either your ~/.bashrc or into the terminal before running bl_download.sh:
	
	function bl {
    		singularity run docker://brainlife/cli $@
	}

### 2. Reslicing diffusion volume derivatives

Second, the volume based diffusion derivatives needed to be resliced and aligned to match the T1w image (reslice_data.sh). This was done as the preprocessing app on brainlife.io only applies the dwi-to-t1 transformation matrix to the header instead of actually transforming the image. This was done using mri_vol2vol from FreeSurfer. The gradients (b-vecs) for the dwi images should still be correct, as the transform was applied to the gradients during preprocessing on brainlife.io.

### 3. Converting HCP_MMP provided by NSD to brainlife datatypes

Third, the HCP_MMP1.mgz parcellations provided in the FreeSurfer directories generated by NSD were converted to surface and volume parcellations to be used in brainlife.io analyses (convert_glasser_nsd.sh). This was done using a combination of FreeSurfer and Connectome Workbench commands.

### 4. Converting cortexmapped giftis to FreeSurfer .mgh files

Fourth, the cortex-mapped diffusion derivatives were converted to a format more suitable for viewing and analyzing with FreeSurfer (convert-to-freesurfer.sh).

### 5. Clean up directory tree

Finally, the overall directory tree was cleaned up to follow the format of the NSD data on aws. Upon cleaning, the data was uploaded to aws via s5 commands.

		
